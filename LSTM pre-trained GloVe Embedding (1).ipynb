{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f19dc9",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "    Load and preprocess the dataset, remove stop words, tokenize the text, and pad sequence to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b78004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4940e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add2d954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo Houston ticket neitu nen kan in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall muhil thei lo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Balisage Markup Conference 2009 Program-a No T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@cynthia_123 i muhil thei lo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Bus bl***y chu ka miss ta!!!!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral  @dannycastillo Houston ticket neitu nen kan in...\n",
       "1  neutral                            cant fall muhil thei lo\n",
       "2  neutral  Balisage Markup Conference 2009 Program-a No T...\n",
       "3  neutral                       @cynthia_123 i muhil thei lo\n",
       "4  neutral                  Bus bl***y chu ka miss ta!!!!!!!!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deef3b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11922, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b6c207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy        4217\n",
       "sadness    2848\n",
       "fear       1748\n",
       "neutral    1578\n",
       "anger      1531\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525ba879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing function \n",
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    # remove links\n",
    "    text = re.sub('https:\\/\\/\\S+', '', text) \n",
    "    # remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n",
    "    # remove next line     \n",
    "    text = re.sub(r'[^ \\w\\.]', '', text) \n",
    "    # remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "# Create a new column called \"Text\" for collecting clean text\n",
    "df['Text'] = df.Text.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f5f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word in  mizo language\n",
    "stop_words  = \"a, i si a min em  le tak te mai e in he u tih ka va keimah keini kan keimahni nangmah nangma nangmahni ngei pawh ani amah a ta chu ni chumi anni an engnge khawi tunge hei sawmi hengte hi tawh nei ti mek leh mahse chuan emaw avang angin hma laiin tan hmunah tu nen lam kalh karah chhungah tlang hmaah hnuah chungah hnuai ah atangin chunglam hnuailam chhung pawn titawp hla zawk tichuan vawikhat hetah sawtah engtikah khawnge engati nge engtin zavai engpawh pahnihin vek tlem belh ber thildang engemawzat chutiang aih lo  ve chauh inang chuvangin aiin lutuk thei duhdan chiah don tur tunah\"\n",
    "\n",
    "# Convert to lower case\n",
    "stop_words = stop_words.lower()\n",
    "\n",
    "# convert string to list\n",
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "\n",
    "\n",
    "stop_word_list = Convert(stop_words)\n",
    "\n",
    "# Remove stop words\n",
    "df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word_list)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c766d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lable encoding emotions\n",
    "df[\"Emotion\"] = df[\"Emotion\"].astype('category').cat.codes # label encoding\n",
    "# Create a dictionary that maps the category code to its corresponding emotion label\n",
    "label_map = dict(enumerate(df[\"Emotion\"].astype('category').cat.categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcddfca",
   "metadata": {},
   "source": [
    "# Map Integer back to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95fb67bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>dannycastillo houston ticket neitu insumdawn t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>cant fall muhil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>balisage markup conference programa no topic m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>muhil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>bus bly miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0        3  dannycastillo houston ticket neitu insumdawn t...\n",
       "1        3                                    cant fall muhil\n",
       "2        3  balisage markup conference programa no topic m...\n",
       "3        3                                              muhil\n",
       "4        3                                       bus bly miss"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571a2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "texts = df['Text'].values\n",
    "labels = df['Emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10eb96d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\raunak\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.54.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\raunak\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\raunak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\raunak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\raunak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\raunak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\raunak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\raunak\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc5840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f6d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e7dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ffe496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the sequence to a fixed length\n",
    "max_length = 10\n",
    "padded_sequence = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a7cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels to one-hot encoded vector\n",
    "num_classes = len(set(labels))\n",
    "one_hot_labels = np.zeros((len(labels),num_classes))\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i, label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c9fdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Golove Word Embedding  file into memory\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coef = np.asarray(values[1:], dtype = 'float32')\n",
    "        embedding_index[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69518a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Matrix by selecting the Glove word Embeddings for the word in our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced2473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embedding_dim, weights = [embedding_matrix], input_length = max_length, trainable = True))\n",
    "model.add(LSTM(units =64, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True))\n",
    "model.add(LSTM(units =32, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2415ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c22920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 100)           1300600   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 64)            42240     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,357,693\n",
      "Trainable params: 1,357,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae243f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "299/299 [==============================] - 25s 53ms/step - loss: 1.4521 - accuracy: 0.3815 - val_loss: 1.2845 - val_accuracy: 0.4734\n",
      "Epoch 2/10\n",
      "299/299 [==============================] - 16s 53ms/step - loss: 1.1291 - accuracy: 0.5627 - val_loss: 1.0874 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "299/299 [==============================] - 17s 57ms/step - loss: 0.8849 - accuracy: 0.6745 - val_loss: 0.9601 - val_accuracy: 0.6516\n",
      "Epoch 4/10\n",
      "299/299 [==============================] - 18s 60ms/step - loss: 0.7237 - accuracy: 0.7482 - val_loss: 0.9309 - val_accuracy: 0.6776\n",
      "Epoch 5/10\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.6173 - accuracy: 0.7909 - val_loss: 0.9355 - val_accuracy: 0.6805\n",
      "Epoch 6/10\n",
      "299/299 [==============================] - 17s 56ms/step - loss: 0.5400 - accuracy: 0.8203 - val_loss: 0.9756 - val_accuracy: 0.6872\n",
      "Epoch 7/10\n",
      "299/299 [==============================] - 17s 56ms/step - loss: 0.4841 - accuracy: 0.8406 - val_loss: 1.0132 - val_accuracy: 0.6843\n",
      "Epoch 8/10\n",
      "299/299 [==============================] - 16s 54ms/step - loss: 0.4318 - accuracy: 0.8576 - val_loss: 1.0544 - val_accuracy: 0.6809\n",
      "Epoch 9/10\n",
      "299/299 [==============================] - 16s 52ms/step - loss: 0.4017 - accuracy: 0.8669 - val_loss: 1.0753 - val_accuracy: 0.6818\n",
      "Epoch 10/10\n",
      "299/299 [==============================] - 16s 53ms/step - loss: 0.3525 - accuracy: 0.8849 - val_loss: 1.1744 - val_accuracy: 0.6679\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_sequence, val_sequence , train_labels, val_labels = train_test_split(padded_sequence, one_hot_labels, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit(train_sequence, train_labels, epochs=10, batch_size = 32, validation_data=(val_sequence, val_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb5f2c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 5ms/step - loss: 1.1744 - accuracy: 0.6679\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(val_sequence, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f62731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for prediction on new data\n",
    "def predict_emotion(text, model, tokenizer, max_length):\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    cleaned_text = ' '.join([word for word in text.split() if word not in (stop_word_list)])\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    \n",
    "    # Pad the sequence to the fixed length\n",
    "    padded_sequence = pad_sequences(tokenized_text, maxlen=max_length)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(padded_sequence)[0]\n",
    "    \n",
    "    # Get the label with the highest probability\n",
    "    label_index = np.argmax(prediction)\n",
    "    \n",
    "    # Map the label index to its corresponding emotion label\n",
    "    emotion = label_map[label_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a2e9d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 541ms/step\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_text = \"ama ka duh loh ber\"\n",
    "predicted_emotion = predict_emotion(new_text, model, tokenizer, max_length)\n",
    "print(predicted_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99882ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
